{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Mohammed Furqan: \n##### March 2020\n\nI would like to thank these guys for their amazing kernels. They are the ones who did the critical part of feature engineering, tuning parameters with clear explanations for beginners to learn:\n\n1. [A study on Regression applied to the Ames dataset](https://www.kaggle.com/juliencs/a-study-on-regression-applied-to-the-ames-dataset) by **Julien Cohen-Solal**\n2. [Stacked Regressions](https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard) by **Serigne**\n3. [XGBoost + Lasso](https://www.kaggle.com/humananalog/xgboost-lasso) by **Human Analog**\n4. [Regularised Linear Models](https://www.kaggle.com/apapiu/regularized-linear-models) by **Alexandru Papiu**\n\nAnd many others. These guys have done a good job in explaining and documenting the codes quite well!"},{"metadata":{},"cell_type":"markdown","source":"## 1. Import libraries and data\n\n\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom scipy.special import boxcox1p\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error, make_scorer\n\n%matplotlib inline\nimport matplotlib.pyplot as plt  # Matlab-style plotting\nimport seaborn as sns\n\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\n\n# Ignore warnings\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn\n\n# For some statistics\nfrom scipy import stats\nfrom scipy.stats import norm, skew \n\n#Limiting floats output to 3 decimal points\npd.set_option('display.float_format', lambda x: '{:.3f}'.format(x))\n\n# Import data\ntrain = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\ntest = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\n\n# Drop ID column\ntrain_ID = train['Id']\ntest_ID = test['Id']\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Data Processing\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop outliers\ntrain.drop(train[train[\"GrLivArea\"] > 4000].index, inplace=True)\n\ntest.loc[1116, \"GarageType\"] = np.nan\n\n# Normalise the target variable since it's skewed distribution\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\ny_train = train.SalePrice.values\n\n# Merge all data for further processing. Drop the target variables\nall_data = pd.concat((train, test)).reset_index(drop=True).drop(['SalePrice'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.1 Missing Values\n\nAfter identifying the columns which have the highest percentage of missing values, we proceed with imputing them."},{"metadata":{"trusted":true},"cell_type":"code","source":"def fillNA(data, var, fill=None, custom=False):\n    if not fill:\n        data_map = {\n            \"float64\": 0,\n            \"int64\": 0,\n            \"object\": \"None\"\n        }\n        data[var] = data[var].fillna(data_map[str(data[var].dtype)])\n    else:\n        if custom:\n            data[var] = data[var].fillna(fill)\n        elif fill == 'mode':\n            # If mode values are to be imputed\n            data[var] = data[var].fillna(data[var].mode()[0])\n    return data\n    \n    \nimpute_vals = [\"BedroomAbvGr\", \"Alley\", \"GarageYrBlt\", \"GarageArea\", \"GarageCars\", 'BsmtFinSF1', 'PoolArea', 'MSSubClass',\n               'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'HalfBath', 'KitchenAbvGr', 'WoodDeckSF',\n               'BsmtHalfBath', 'MasVnrType', 'MasVnrArea', 'MiscVal', 'OpenPorchSF', 'ScreenPorch', 'TotRmsAbvGrd', \"EnclosedPorch\"]\n\n# Impute NAs with default '0' or 'None' depending on the feature's data type\nfor var in impute_vals:\n    all_data = fillNA(all_data, var)\n    \n# Impute the NAs with respective mode values\nimpute_mode = ['MSZoning', 'Electrical', 'Exterior1st', 'Exterior2nd', 'SaleType']\nfor var in impute_mode:\n    all_data = fillNA(all_data, var, fill='mode')\n    \n# Impute with custom default values\nimpute_custom = {\n    \"CentralAir\": \"N\",\n    \"Condition1\": \"Norm\",\n    \"Condition1\": \"Norm\",\n    \"Functional\": \"Typ\",\n    \"HeatingQC\" : \"TA\",\n    \"LotShape\"  : \"Reg\",\n    \"PavedDrive\": \"N\",\n    \"PoolQC\"    : \"No\",\n    \"BsmtQual\"  : \"No\",\n    \"BsmtCond\"  : \"No\",\n    \"BsmtExposure\"  : \"No\",\n    \"BsmtFinType1\"  : \"No\",\n    \"BsmtFinType2\"  : \"No\",\n    \"ExterCond\"  : \"No\",\n    \"ExterQual\"  : \"No\",\n    \"Fence\"  : \"No\",\n    \"FireplaceQu\"  : \"No\",\n    \"GarageType\"  : \"No\",\n    \"GarageFinish\"  : \"No\",\n    \"GarageQual\"  : \"No\",\n    \"GarageCond\"  : \"No\",\n    \"KitchenQual\"  : \"TA\",\n    \"MiscFeature\"  : \"No\",\n    \"PavedDrive\" : \"N\",\n    \"SaleCondition\": \"Normal\"\n}\n\nfor var in impute_custom:\n    all_data = fillNA(all_data, var, impute_custom[var], custom=True)\n    \n    \n# Some Unique cases\nall_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\") # As per the data description NA means 'typical'\nall_data = all_data.drop(['Utilities'], axis=1) # Since all values are same\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median())) # LotFrontage by neighborhood","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### 2.2 Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert some of the numeric values to strings as they are in-face categories\nfor var in ['MSSubClass', 'YrSold', 'OverallCond', 'MoSold']:\n    all_data[var] = all_data[var].astype(str)\n    \n# Encode some categorical features as ordered numbers when there is information in the order\nall_data = all_data.replace({\"Alley\" : {\"No\" : 0, \"Grvl\" : 1, \"Pave\" : 2},\n                       \"BsmtCond\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"BsmtExposure\" : {\"No\" : 0, \"Mn\" : 1, \"Av\": 2, \"Gd\" : 3},\n                       \"BsmtFinType1\" : {\"No\" : 0, \"Unf\" : 1, \"LwQ\": 2, \"Rec\" : 3, \"BLQ\" : 4, \"ALQ\" : 5, \"GLQ\" : 6},\n                       \"BsmtFinType2\" : {\"No\" : 0, \"Unf\" : 1, \"LwQ\": 2, \"Rec\" : 3, \"BLQ\" : 4, \"ALQ\" : 5, \"GLQ\" : 6},\n                       \"BsmtQual\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"ExterCond\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\": 4, \"Ex\" : 5},\n                       \"ExterQual\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\": 4, \"Ex\" : 5},\n                       \"FireplaceQu\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"Functional\" : {\"No\" : 0, \"Sal\" : 1, \"Sev\" : 2, \"Maj2\" : 3, \"Maj1\" : 4, \"Mod\": 5, \"Min2\" : 6, \"Min1\" : 7, \"Typ\" : 8},\n                       \"GarageCond\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"GarageQual\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"HeatingQC\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"KitchenQual\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"LandSlope\" : {\"Sev\" : 1, \"Mod\" : 2, \"Gtl\" : 3},\n                       \"LotShape\" : {\"IR3\" : 1, \"IR2\" : 2, \"IR1\" : 3, \"Reg\" : 4},\n                       \"PavedDrive\" : {\"N\" : 0, \"P\" : 1, \"Y\" : 2},\n                       \"PoolQC\" : {\"No\" : 0, \"Fa\" : 1, \"TA\" : 2, \"Gd\" : 3, \"Ex\" : 4},\n                       \"Street\" : {\"Grvl\" : 1, \"Pave\" : 2},\n                       \"Utilities\" : {\"ELO\" : 1, \"NoSeWa\" : 2, \"NoSewr\" : 3, \"AllPub\" : 4}}\n                     )\n\n\n# Create new features\n# 1* Simplifications of existing features\nall_data[\"SimplOverallQual\"] = all_data.OverallQual.replace({1 : 1, 2 : 1, 3 : 1,  4 : 2, 5 : 2, 6 : 2,7 : 3, 8 : 3, 9 : 3, 10 : 3})\nall_data[\"SimplOverallCond\"] = all_data.OverallCond.replace({1 : 1, 2 : 1, 3 : 1,  4 : 2, 5 : 2, 6 : 2,7 : 3, 8 : 3, 9 : 3, 10 : 3})\nall_data[\"SimplPoolQC\"] = all_data.PoolQC.replace({1 : 1, 2 : 1,3 : 2, 4 : 2})\nall_data[\"SimplGarageCond\"] = all_data.GarageCond.replace({1 : 1, 4 : 2, 5 : 2})\nall_data[\"SimplGarageQual\"] = all_data.GarageQual.replace({1 : 1, 4 : 2, 5 : 2})\nall_data[\"SimplFireplaceQu\"] = all_data.FireplaceQu.replace({1 : 1, 4 : 2, 5 : 2})\nall_data[\"SimplFireplaceQu\"] = all_data.FireplaceQu.replace({1 : 1, 4 : 2, 5 : 2})\nall_data[\"SimplFunctional\"] = all_data.Functional.replace({1 : 1, 2 : 1,3 : 2, 4 : 2,5 : 3, 6 : 3, 7 : 3, 8 : 4})\nall_data[\"SimplKitchenQual\"] = all_data.KitchenQual.replace({1 : 1, 4 : 2, 5 : 2})\nall_data[\"SimplHeatingQC\"] = all_data.HeatingQC.replace({1 : 1,4 : 2, 5 : 2})\nall_data[\"SimplBsmtFinType1\"] = all_data.BsmtFinType1.replace({1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2, 6 : 2 })\nall_data[\"SimplBsmtFinType2\"] = all_data.BsmtFinType2.replace({1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2, 6 : 2})\nall_data[\"SimplBsmtCond\"] = all_data.BsmtCond.replace({1 : 1, 4 : 2, 5 : 2})\nall_data[\"SimplBsmtQual\"] = all_data.BsmtQual.replace({1 : 1,4 : 2, 5 : 2})\nall_data[\"SimplExterCond\"] = all_data.ExterCond.replace({1 : 1,4 : 2, 5 : 2})\nall_data[\"SimplExterQual\"] = all_data.ExterQual.replace({1 : 1,4 : 2, 5 : 2})\n\n# 2* Combinations of existing features\nall_data[\"OverallGrade\"] = all_data[\"OverallQual\"] * all_data[\"OverallCond\"]\nall_data[\"GarageGrade\"] = all_data[\"GarageQual\"] * all_data[\"GarageCond\"]\nall_data[\"ExterGrade\"] = all_data[\"ExterQual\"] * all_data[\"ExterCond\"]\nall_data[\"KitchenScore\"] = all_data[\"KitchenAbvGr\"] * all_data[\"KitchenQual\"]\nall_data[\"FireplaceScore\"] = all_data[\"Fireplaces\"] * all_data[\"FireplaceQu\"]\nall_data[\"GarageScore\"] = all_data[\"GarageArea\"] * all_data[\"GarageQual\"]\nall_data[\"PoolScore\"] = all_data[\"PoolArea\"] * all_data[\"PoolQC\"]\nall_data[\"SimplOverallGrade\"] = all_data[\"SimplOverallQual\"] * all_data[\"SimplOverallCond\"]\nall_data[\"SimplExterGrade\"] = all_data[\"SimplExterQual\"] * all_data[\"SimplExterCond\"]\nall_data[\"SimplPoolScore\"] = all_data[\"PoolArea\"] * all_data[\"SimplPoolQC\"]\nall_data[\"SimplGarageScore\"] = all_data[\"GarageArea\"] * all_data[\"SimplGarageQual\"]\nall_data[\"SimplFireplaceScore\"] = all_data[\"Fireplaces\"] * all_data[\"SimplFireplaceQu\"]\nall_data[\"SimplKitchenScore\"] = all_data[\"KitchenAbvGr\"] * all_data[\"SimplKitchenQual\"]\nall_data[\"TotalBath\"] = all_data[\"BsmtFullBath\"] + (0.5 * all_data[\"BsmtHalfBath\"]) + \\\nall_data[\"FullBath\"] + (0.5 * all_data[\"HalfBath\"])\nall_data[\"AllSF\"] = all_data[\"GrLivArea\"] + all_data[\"TotalBsmtSF\"]\nall_data[\"AllFlrsSF\"] = all_data[\"1stFlrSF\"] + all_data[\"2ndFlrSF\"]\nall_data[\"AllPorchSF\"] = all_data[\"OpenPorchSF\"] + all_data[\"EnclosedPorch\"] + \\\nall_data[\"3SsnPorch\"] + all_data[\"ScreenPorch\"]\nall_data[\"HasMasVnr\"] = all_data.MasVnrType.replace({\"BrkCmn\" : 1, \"BrkFace\" : 1, \"CBlock\" : 1, \"Stone\" : 1, \"None\" : 0})\nall_data[\"BoughtOffPlan\"] = all_data.SaleCondition.replace({\"Abnorml\" : 0, \"Alloca\" : 0, \"AdjLand\" : 0, \"Family\" : 0, \"Normal\" : 0, \"Partial\" : 1})\n\n# 3* Polynomials on the top 10 existing features\nall_data[\"OverallQual-s2\"] = all_data[\"OverallQual\"] ** 2\nall_data[\"OverallQual-s3\"] = all_data[\"OverallQual\"] ** 3\nall_data[\"OverallQual-Sq\"] = np.sqrt(all_data[\"OverallQual\"])\nall_data[\"AllSF-2\"] = all_data[\"AllSF\"] ** 2\nall_data[\"AllSF-3\"] = all_data[\"AllSF\"] ** 3\nall_data[\"AllSF-Sq\"] = np.sqrt(all_data[\"AllSF\"])\nall_data[\"AllFlrsSF-2\"] = all_data[\"AllFlrsSF\"] ** 2\nall_data[\"AllFlrsSF-3\"] = all_data[\"AllFlrsSF\"] ** 3\nall_data[\"AllFlrsSF-Sq\"] = np.sqrt(all_data[\"AllFlrsSF\"])\nall_data[\"GrLivArea-2\"] = all_data[\"GrLivArea\"] ** 2\nall_data[\"GrLivArea-3\"] = all_data[\"GrLivArea\"] ** 3\nall_data[\"GrLivArea-Sq\"] = np.sqrt(all_data[\"GrLivArea\"])\nall_data[\"SimplOverallQual-s2\"] = all_data[\"SimplOverallQual\"] ** 2\nall_data[\"SimplOverallQual-s3\"] = all_data[\"SimplOverallQual\"] ** 3\nall_data[\"SimplOverallQual-Sq\"] = np.sqrt(all_data[\"SimplOverallQual\"])\nall_data[\"ExterQual-2\"] = all_data[\"ExterQual\"] ** 2\nall_data[\"ExterQual-3\"] = all_data[\"ExterQual\"] ** 3\nall_data[\"ExterQual-Sq\"] = np.sqrt(all_data[\"ExterQual\"])\nall_data[\"GarageCars-2\"] = all_data[\"GarageCars\"] ** 2\nall_data[\"GarageCars-3\"] = all_data[\"GarageCars\"] ** 3\nall_data[\"GarageCars-Sq\"] = np.sqrt(all_data[\"GarageCars\"])\nall_data[\"TotalBath-2\"] = all_data[\"TotalBath\"] ** 2\nall_data[\"TotalBath-3\"] = all_data[\"TotalBath\"] ** 3\nall_data[\"TotalBath-Sq\"] = np.sqrt(all_data[\"TotalBath\"])\nall_data[\"KitchenQual-2\"] = all_data[\"KitchenQual\"] ** 2\nall_data[\"KitchenQual-3\"] = all_data[\"KitchenQual\"] ** 3\nall_data[\"KitchenQual-Sq\"] = np.sqrt(all_data[\"KitchenQual\"])\nall_data[\"GarageScore-2\"] = all_data[\"GarageScore\"] ** 2\nall_data[\"GarageScore-3\"] = all_data[\"GarageScore\"] ** 3\nall_data[\"GarageScore-Sq\"] = np.sqrt(all_data[\"GarageScore\"])\n\n# Adding total sqfootage feature \nall_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n\n# Normalise skewed numeric features\ncategorical_features = all_data.select_dtypes(include = [\"object\"]).columns\nnumerical_features = all_data.select_dtypes(exclude = [\"object\"]).columns\ntrain_num = all_data[numerical_features]\ntrain_cat = all_data[categorical_features]\n\nskewness = train_num.apply(lambda x: skew(x))\nskewness = skewness[abs(skewness) > 0.75]\nskewed_features = skewness.index\ntrain_num[skewed_features] = boxcox1p(train_num[skewed_features], 0.15)\n\n# Get one-hot representation for categorical variables\ntrain_cat = pd.get_dummies(train_cat)\n\nall_data = pd.concat([train_num, train_cat], axis = 1)\n\ntrain = all_data[:train.shape[0]]\ntest = all_data[train.shape[0]:]\n\n# Standardize numerical features\nscale = StandardScaler()\ntrain.loc[:, numerical_features] = scale.fit_transform(train.loc[:, numerical_features])\ntest.loc[:, numerical_features] = scale.transform(test.loc[:, numerical_features])\n\ndrop_cols = [\"Exterior1st_ImStucc\", \"Exterior1st_Stone\", \"Exterior2nd_Other\",\"HouseStyle_2.5Fin\", \n             \"RoofMatl_Membran\", \"RoofMatl_Metal\", \"RoofMatl_Roll\",\n             \"Condition2_RRAe\", \"Condition2_RRAn\", \"Condition2_RRNn\",\n             \"Heating_Floor\", \"Heating_OthW\", \"MSSubClass_150\", \n             \"Electrical_Mix\", \"MiscFeature_TenC\", \"Condition2_PosN\", \"MSZoning_C (all)\", \"MSSubClass_160\"\n            ]\ntrain.drop(drop_cols, axis=1, inplace=True)\ntest.drop(drop_cols, axis=1, inplace=True)\n\nprint(train.shape, test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Modelling"},{"metadata":{},"cell_type":"markdown","source":"### 3.1 Base Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import ElasticNet, Lasso, BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb\n\n#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    print(train.shape, y_train.shape)\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)\n\n\nlasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\nENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\nKRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\nGBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\nmodel_xgb = xgb.XGBRegressor(colsample_bytree=0.2, gamma=0, \n                             learning_rate=0.01, max_depth=4, \n                             min_child_weight=1.5, n_estimators=7200,\n                             reg_alpha=0.9, reg_lambda=0.6,\n                             subsample=0.2, silent=1,\n                             seed = 25)\nmodel_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.2 Stacking Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\nclass StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)\n    \naveraged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR, model_xgb, model_lgb), meta_model = lasso)\nscore = rmsle_cv(averaged_models)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\naveraged_models.fit(train.values, y_train)\nstacked_pred = np.expm1(averaged_models.predict(test.values))\n\n# Save the predictions\nfinal_prediction = pd.DataFrame({\"Id\": test_ID, \"SalePrice\": stacked_pred})\nfinal_prediction.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}